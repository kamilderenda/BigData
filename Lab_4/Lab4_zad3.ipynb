{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51282d5a-1cc7-4bd5-9299-7f4a5a72a44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n|RowID|     FName|Salary|\n+-----+----------+------+\n|    1|2011-01-01|   500|\n|    1|2011-01-15|    50|\n|    1|2011-01-22|   250|\n|    1|2011-01-24|    75|\n|    1|2011-01-26|   125|\n|    1|2011-01-28|   175|\n|    2|2011-01-01|   500|\n|    2|2011-01-15|    50|\n|    2|2011-01-22|    25|\n|    2|2011-01-23|   125|\n|    2|2011-01-26|   200|\n|    2|2011-01-29|   250|\n|    3|2011-01-01|   500|\n|    3|2011-01-15|    50|\n|    3|2011-01-22|  5000|\n|    3|2011-01-25|   550|\n|    3|2011-01-27|    95|\n|    3|2011-01-30|  2500|\n+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, '2011-01-01', 500), (1, '2011-01-15', 50), (1, '2011-01-22', 250),\n",
    "    (1, '2011-01-24', 75), (1, '2011-01-26', 125), (1, '2011-01-28', 175),\n",
    "    (2, '2011-01-01', 500), (2, '2011-01-15', 50), (2, '2011-01-22', 25),\n",
    "    (2, '2011-01-23', 125), (2, '2011-01-26', 200), (2, '2011-01-29', 250),\n",
    "    (3, '2011-01-01', 500), (3, '2011-01-15', 50), (3, '2011-01-22', 5000),\n",
    "    (3, '2011-01-25', 550), (3, '2011-01-27', 95), (3, '2011-01-30', 2500)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = ['RowID','FName','Salary'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002d4e48-a78c-4ec5-ba38-ce1076fa106c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>RowID</th><th>FName</th><th>Salary</th></tr></thead><tbody><tr><td>1</td><td>George</td><td>800</td></tr><tr><td>2</td><td>Sam</td><td>950</td></tr><tr><td>3</td><td>Diane</td><td>1100</td></tr><tr><td>4</td><td>Nicholas</td><td>1250</td></tr><tr><td>5</td><td>Samuel</td><td>1250</td></tr><tr><td>6</td><td>Patricia</td><td>1300</td></tr><tr><td>7</td><td>Brian</td><td>1500</td></tr><tr><td>8</td><td>Thomas</td><td>1600</td></tr><tr><td>9</td><td>Fran</td><td>2450</td></tr><tr><td>10</td><td>Debbie</td><td>2850</td></tr><tr><td>11</td><td>Mark</td><td>2975</td></tr><tr><td>12</td><td>James</td><td>3000</td></tr><tr><td>13</td><td>Cynthia</td><td>3000</td></tr><tr><td>14</td><td>Christopher</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "George",
         800
        ],
        [
         2,
         "Sam",
         950
        ],
        [
         3,
         "Diane",
         1100
        ],
        [
         4,
         "Nicholas",
         1250
        ],
        [
         5,
         "Samuel",
         1250
        ],
        [
         6,
         "Patricia",
         1300
        ],
        [
         7,
         "Brian",
         1500
        ],
        [
         8,
         "Thomas",
         1600
        ],
        [
         9,
         "Fran",
         2450
        ],
        [
         10,
         "Debbie",
         2850
        ],
        [
         11,
         "Mark",
         2975
        ],
        [
         12,
         "James",
         3000
        ],
        [
         13,
         "Cynthia",
         3000
        ],
        [
         14,
         "Christopher",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "RowID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logical_data = [\n",
    "(1,'George', 800),\n",
    "(2,'Sam', 950),\n",
    "(3,'Diane', 1100),\n",
    "(4,'Nicholas', 1250),\n",
    "(5,'Samuel', 1250),\n",
    "(6,'Patricia', 1300),\n",
    "(7,'Brian', 1500),\n",
    "(8,'Thomas', 1600),\n",
    "(9,'Fran', 2450),\n",
    "(10,'Debbie', 2850),\n",
    "(11,'Mark', 2975),\n",
    "(12,'James', 3000),\n",
    "(13,'Cynthia', 3000),\n",
    "(14,'Christopher', 5000)\n",
    "]\n",
    "\n",
    "logical_df = spark.createDataFrame(logical_data, [\"RowID\", \"FName\", \"Salary\"])\n",
    "display(logical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07135cc-b6f5-49b1-a160-716b15105706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+-----------+\n|RowID|     FName|Salary|RunTotalAmt|\n+-----+----------+------+-----------+\n|    1|2011-01-01|   500|        500|\n|    1|2011-01-15|    50|        550|\n|    1|2011-01-22|   250|        800|\n|    1|2011-01-24|    75|        875|\n|    1|2011-01-26|   125|       1000|\n|    1|2011-01-28|   175|       1175|\n|    2|2011-01-01|   500|        500|\n|    2|2011-01-15|    50|        550|\n|    2|2011-01-22|    25|        575|\n|    2|2011-01-23|   125|        700|\n|    2|2011-01-26|   200|        900|\n|    2|2011-01-29|   250|       1150|\n|    3|2011-01-01|   500|        500|\n|    3|2011-01-15|    50|        550|\n|    3|2011-01-22|  5000|       5550|\n|    3|2011-01-25|   550|       6100|\n|    3|2011-01-27|    95|       6195|\n|    3|2011-01-30|  2500|       8695|\n+-----+----------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# running total of all transactions\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "def add_running_total(df, id_col: str, date_col: str, price_col: str):\n",
    "    window_spec = Window.partitionBy(id_col).orderBy(date_col).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    \n",
    "    return df.withColumn(\"RunTotalAmt\", f.sum(price_col).over(window_spec))\n",
    "\n",
    "df_with_total = add_running_total(df, \"RowID\", \"FName\", \"Salary\")\n",
    "df_with_total.show()\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a375dd-9864-4a61-b515-44794457102b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+------------------+----------+---------+---------+-----------+\n|RowID|     FName|Salary|            RunAvg|RunTranQty|RunMinAmt|RunMaxAmt|RunTotalAmt|\n+-----+----------+------+------------------+----------+---------+---------+-----------+\n|    1|2011-01-01|   500|             500.0|         1|      500|      500|        500|\n|    1|2011-01-15|    50|             275.0|         2|       50|      500|        550|\n|    1|2011-01-22|   250| 266.6666666666667|         3|       50|      500|        800|\n|    1|2011-01-24|    75|            218.75|         4|       50|      500|        875|\n|    1|2011-01-26|   125|             200.0|         5|       50|      500|       1000|\n|    1|2011-01-28|   175|195.83333333333334|         6|       50|      500|       1175|\n|    2|2011-01-01|   500|             500.0|         1|      500|      500|        500|\n|    2|2011-01-15|    50|             275.0|         2|       50|      500|        550|\n|    2|2011-01-22|    25|191.66666666666666|         3|       25|      500|        575|\n|    2|2011-01-23|   125|             175.0|         4|       25|      500|        700|\n|    2|2011-01-26|   200|             180.0|         5|       25|      500|        900|\n|    2|2011-01-29|   250|191.66666666666666|         6|       25|      500|       1150|\n|    3|2011-01-01|   500|             500.0|         1|      500|      500|        500|\n|    3|2011-01-15|    50|             275.0|         2|       50|      500|        550|\n|    3|2011-01-22|  5000|            1850.0|         3|       50|     5000|       5550|\n|    3|2011-01-25|   550|            1525.0|         4|       50|     5000|       6100|\n|    3|2011-01-27|    95|            1239.0|         5|       50|     5000|       6195|\n|    3|2011-01-30|  2500|1449.1666666666667|         6|       50|     5000|       8695|\n+-----+----------+------+------------------+----------+---------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "def fun1(df, id_col: str, date_col: str, trans: str):\n",
    "    window = Window.partitionBy(id_col).orderBy(date_col)\n",
    "\n",
    "    return df.withColumn(\"RunAvg\", f.avg(trans).over(window))\\\n",
    "        .withColumn(\"RunTranQty\", f.count(\"*\").over(window))\\\n",
    "        .withColumn(\"RunMinAmt\", f.min(trans).over(window))\\\n",
    "        .withColumn(\"RunMaxAmt\", f.max(trans).over(window))\\\n",
    "        .withColumn(\"RunTotalAmt\", f.sum(trans).over(window))\\\n",
    "        .orderBy(id_col, date_col) \n",
    "\n",
    "df_with_total = fun1(df, \"RowID\", \"FName\", \"Salary\")\n",
    "df_with_total.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8adf6b6-f493-4352-a3d6-5ff4956b1b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+------------------+--------+--------+--------+---------+\n|RowID|     FName|Salary|            RunAvg|SlideQty|SlideMin|SlideMax|RowNumber|\n+-----+----------+------+------------------+--------+--------+--------+---------+\n|    1|2011-01-01|   500|             500.0|       1|     500|     500|      500|\n|    1|2011-01-15|    50|             275.0|       2|      50|     500|      550|\n|    1|2011-01-22|   250| 266.6666666666667|       3|      50|     500|      800|\n|    1|2011-01-24|    75|             125.0|       3|      50|     250|      875|\n|    1|2011-01-26|   125|             150.0|       3|      75|     250|     1000|\n|    1|2011-01-28|   175|             125.0|       3|      75|     175|     1175|\n|    2|2011-01-01|   500|             500.0|       1|     500|     500|      500|\n|    2|2011-01-15|    50|             275.0|       2|      50|     500|      550|\n|    2|2011-01-22|    25|191.66666666666666|       3|      25|     500|      575|\n|    2|2011-01-23|   125| 66.66666666666667|       3|      25|     125|      700|\n|    2|2011-01-26|   200|116.66666666666667|       3|      25|     200|      900|\n|    2|2011-01-29|   250|191.66666666666666|       3|     125|     250|     1150|\n|    3|2011-01-01|   500|             500.0|       1|     500|     500|      500|\n|    3|2011-01-15|    50|             275.0|       2|      50|     500|      550|\n|    3|2011-01-22|  5000|            1850.0|       3|      50|    5000|     5550|\n|    3|2011-01-25|   550|1866.6666666666667|       3|      50|    5000|     6100|\n|    3|2011-01-27|    95|1881.6666666666667|       3|      95|    5000|     6195|\n|    3|2011-01-30|  2500|1048.3333333333333|       3|      95|    2500|     8695|\n+-----+----------+------+------------------+--------+--------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def fun2(df, id_col: str, date_col: str, trans: str):\n",
    "    window = Window.partitionBy(id_col).orderBy(date_col)\n",
    "    window2=Window.partitionBy(id_col).orderBy(date_col).rowsBetween(-2, 0)\n",
    "\n",
    "\n",
    "    return df.withColumn(\"RunAvg\", f.avg(trans).over(window2))\\\n",
    "        .withColumn(\"SlideQty\", f.count(\"*\").over(window2))\\\n",
    "        .withColumn(\"SlideMin\", f.min(trans).over(window2))\\\n",
    "        .withColumn(\"SlideMax\", f.max(trans).over(window2))\\\n",
    "        .withColumn(\"RowNumber\", f.sum(trans).over(window))\\\n",
    "        .orderBy([id_col, date_col,\"RowNumber\"] ) \n",
    "\n",
    "df_with_total = fun2(df, \"RowID\", \"FName\", \"Salary\")\n",
    "df_with_total.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f460aa89-c9f9-4da4-aa55-8820a138498b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+---------+----------+\n|RowID|      FName|Salary|SumByRows|SumByRange|\n+-----+-----------+------+---------+----------+\n|    1|     George|   800|      800|       800|\n|    2|        Sam|   950|     1750|      1750|\n|    3|      Diane|  1100|     2850|      2850|\n|    4|   Nicholas|  1250|     4100|      5350|\n|    5|     Samuel|  1250|     5350|      5350|\n|    6|   Patricia|  1300|     6650|      6650|\n|    7|      Brian|  1500|     8150|      8150|\n|    8|     Thomas|  1600|     9750|      9750|\n|    9|       Fran|  2450|    12200|     12200|\n|   10|     Debbie|  2850|    15050|     15050|\n|   11|       Mark|  2975|    18025|     18025|\n|   12|      James|  3000|    21025|     24025|\n|   13|    Cynthia|  3000|    24025|     24025|\n|   14|Christopher|  5000|    29025|     29025|\n+-----+-----------+------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "def fun3(df, id_col: str, name_col: str, salary: str):\n",
    "    window = Window.orderBy('Salary').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    window2=Window.orderBy('Salary').rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    return df.withColumn(\"SumByRows\", f.sum(salary).over(window))\\\n",
    "    .withColumn(\"SumByRange\", f.sum(salary).over(window2))\\\n",
    "    .orderBy(id_col)\n",
    "\n",
    "df_with_total = fun3(logical_df, \"RowID\", \"FName\", \"Salary\")\n",
    "df_with_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a4a48c-9c40-432f-8db9-65c2154fd315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab4_zad3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}